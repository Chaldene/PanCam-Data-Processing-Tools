{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitprocessorconda4f323e9960824ce38e2301e05c58c044",
   "display_name": "Python 3.7.6 64-bit ('Processor': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import filecmp\n",
    "from bitstruct import unpack_from as upf\n",
    "\n",
    "import PC_Fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Yeah is a file\n"
    }
   ],
   "source": [
    "nsvf_file = Path(r\"C:\\Users\\ucasbwh\\OneDrive - University College London\\PanCam Documents\\SWIS\\200131 SWIS L2 3.1.12\\NSVF Logs\\PanCam_v2.0B_Delay10_RUN_1\\Test.log\")\n",
    "nsvf_file = Path(r\"C:\\Users\\ucasbwh\\OneDrive - University College London\\PanCam Documents\\SWIS\\200131 SWIS L2 3.1.12\\NSVF Logs\\PanCam_v2.0B_Delay10_RUN_1\\Router_A_packet.log\")\n",
    "\n",
    "if not nsvf_file.exists():\n",
    "    print(\"Yeah not a file\")\n",
    "else:\n",
    "    print(\"Yeah is a file\")\n",
    "\n",
    "test_file = Path(r\"C:\\Users\\ucasbwh\\OneDrive - University College London\\PanCam Documents\\SWIS\\200131 SWIS L2 3.1.12\\NSVF Logs\\PanCam_v2.0B_Delay10_RUN_1\\Output.log\")\n",
    "hs_file = Path(r\"C:\\Users\\ucasbwh\\OneDrive - University College London\\PanCam Documents\\SWIS\\200131 SWIS L2 3.1.12\\NSVF Logs\\PanCam_v2.0B_Delay10_RUN_1\\H&S.log\")\n",
    "hk_file = Path(r\"C:\\Users\\ucasbwh\\OneDrive - University College London\\PanCam Documents\\SWIS\\200131 SWIS L2 3.1.12\\NSVF Logs\\PanCam_v2.0B_Delay10_RUN_1\\HK.log\")\n",
    "tc_file = Path(r\"C:\\Users\\ucasbwh\\OneDrive - University College London\\PanCam Documents\\SWIS\\200131 SWIS L2 3.1.12\\NSVF Logs\\PanCam_v2.0B_Delay10_RUN_1\\TC.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Done\n"
    }
   ],
   "source": [
    "# Check initial format is as expected, #[IN=..], #[SZ=..], [EOP] at end\n",
    "\n",
    "# Checks to perform\n",
    "# Is a file and readable\n",
    "# That each line:\n",
    "#       ends with \"[EOP]\"\n",
    "#       contains [IN=$I$] with $I$ some integer\n",
    "#       contains [SZ=$I$] with $I$ some integer\n",
    "#       that the size in [SZ..] matches packet size\n",
    "\n",
    "in_re = re.compile(\"^\\[IN=[0-9]+\\]$\")\n",
    "sz_re = re.compile(\"^\\[SZ=[0-9]+\\]$\")\n",
    "\n",
    "# First check that #[IN=..]\n",
    "outputfile = open(test_file, 'w')\n",
    "linewriter = csv.writer(outputfile, delimiter=' ', lineterminator='\\r')\n",
    "hsfile = open(hs_file, 'w')\n",
    "hs_writer = csv.writer(hsfile, delimiter=' ', lineterminator='\\r')\n",
    "hkfile = open(hk_file, 'w')\n",
    "hk_writer = csv.writer(hkfile, delimiter=' ', lineterminator='\\r')\n",
    "tcfile = open(tc_file, 'w')\n",
    "tc_writer = csv.writer(tcfile, delimiter=' ', lineterminator='\\r')\n",
    "\n",
    "with open(nsvf_file) as logfile:\n",
    "    reader = csv.reader(logfile, delimiter=' ')\n",
    "    for row in reader:\n",
    "        #print(row)\n",
    "        if row[-1] != '[EOP]': print(\"Error Found\")\n",
    "        if not in_re.match(row[1]): print(\"No Match for [IN]\")\n",
    "        if not sz_re.match(row[2]): print(\"No Match for [SZ]\")\n",
    "        row_size = int(row[2][4:-1])\n",
    "        if row_size != len(row[4:-1]): print(\"Packet does not match expected size\")\n",
    "\n",
    "        # Filter by packet types\n",
    "        # If SZ = 45 and start = [80 00 00 00] then H&S\n",
    "        # Then determine TM Header\n",
    "\n",
    "        log_addr = int(row[8], 16)\n",
    "        if log_addr == 0x41:\n",
    "            if row_size == 45:\n",
    "                row_reduced = row\n",
    "                row_reduced[1:4] = []\n",
    "                row_reduced[-1:] = []\n",
    "                row_reduced[0] += ';'\n",
    "                hs_writer.writerow(row)\n",
    "            elif row_size == 8:\n",
    "                tc_writer.writerow(row)\n",
    "            else:\n",
    "                tm_type = int(row[17], 16) >> 2\n",
    "                if tm_type < 2:\n",
    "                    hk_writer.writerow(row)\n",
    "                else:\n",
    "                    linewriter.writerow(row)\n",
    "\n",
    "outputfile.close()\n",
    "hsfile.close()\n",
    "hkfile.close()\n",
    "tcfile.close()\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpc_hk_addr = 0x80000000\n",
    "pc_hk_lens = [0, 72, 88]\n",
    "pc_sci_addr = 0xC0000000\n",
    "pc_sci_len  = [0x200030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "HS HK Count not increasing, %d occurances 1\nOccurance\n                   Time                                                RAW  \\\n1104  [515:838:009:386]   21 01 0C 00 41 00 B9 00 00 00 20 D5 80 00 00 ...   \n1105  [533:620:009:386]   21 01 0C 00 41 00 31 00 00 00 20 DF 80 00 00 ...   \n1106  [533:820:009:386]   21 01 0C 00 41 00 32 00 00 00 20 25 80 00 00 ...   \n\n         HK_Addr  HK_Len  HK_Cnt    Sci_Addr  Sci_Len  LDT  Sci_Cnt  \n1104  2147483648      88     219  3221225472  2097200    0        1  \n1105  2147483648       0       0  3221225472  2097200    0        0  \n1106  2147483648       0       0  3221225472  2097200    0        0  \nHS Sci count not increasing, %d occurances 1\nOccurance\n                   Time                                                RAW  \\\n1104  [515:838:009:386]   21 01 0C 00 41 00 B9 00 00 00 20 D5 80 00 00 ...   \n1105  [533:620:009:386]   21 01 0C 00 41 00 31 00 00 00 20 DF 80 00 00 ...   \n1106  [533:820:009:386]   21 01 0C 00 41 00 32 00 00 00 20 25 80 00 00 ...   \n\n         HK_Addr  HK_Len  HK_Cnt    Sci_Addr  Sci_Len  LDT  Sci_Cnt  \n1104  2147483648      88     219  3221225472  2097200    0        1  \n1105  2147483648       0       0  3221225472  2097200    0        0  \n1106  2147483648       0       0  3221225472  2097200    0        0  \nA total of %d images generated over session 2\n"
    }
   ],
   "source": [
    "# HS Verify\n",
    "# Check that the HK address is always 0x80 00 00 00\n",
    "# Check that the HK counter is always increasing\n",
    "# Check that the Sci address is alwa***ys 0xC0 00 00 00\n",
    "# Check that the Sci counter increases\n",
    "# Count number of images generated\n",
    "# Ensure LDT count is always <7\n",
    "\n",
    "hs_head = ['Time', 'RAW']\n",
    "hs = pd.read_csv(hs_file, sep=';', header=None, names=hs_head)\n",
    "\n",
    "#DL['RAW'] = DL.RAW_DATA.apply(lambda x: x[38:-4])\n",
    "\n",
    "raw = hs['RAW'].apply(lambda x: bytearray.fromhex(x))\n",
    "\n",
    "# HS Decode\n",
    "hs['HK_Addr'] = PandUPF(raw, 'u32', 12, 0)\n",
    "hs['HK_Len'] = PandUPF(raw, 'u16', 16, 0)\n",
    "hs['HK_Cnt'] = PandUPF(raw, 'u16', 18, 0)\n",
    "hs['Sci_Addr'] = PandUPF(raw, 'u32', 20, 0)\n",
    "hs['Sci_Len'] = PandUPF(raw, 'u24', 24, 0)\n",
    "hs['LDT'] = PandUPF(raw, 'u8', 27, 0)\n",
    "hs['Sci_Cnt'] = PandUPF(raw, 'u16', 28, 0)\n",
    "\n",
    "# Verify HK Address is always 0x80 00 00 00\n",
    "# Check first value is as expected\n",
    "verify = pd.DataFrame()\n",
    "err_df = pd.DataFrame()\n",
    "verify['HK_Addr'] = ~(hs['HK_Addr'] == pc_hk_addr)\n",
    "err_df = hs[verify['HK_Addr']] \n",
    "if err_df.shape[0] != 0:\n",
    "    print(\"HS HK Address not as expected %d occurances\", err_df.shape[0])\n",
    "    print(err_df)\n",
    "\n",
    "# Verify the HK Length is either 72 or 88 bytes\n",
    "verify['HK_Len'] = ~hs['HK_Len'].isin(pc_hk_lens)\n",
    "err_df = hs[verify['HK_Len']]\n",
    "if err_df.shape[0] != 0:\n",
    "    print(\"HS HK unexpected length %d occurances\", err_df.shape[0])\n",
    "    print(err_df)\n",
    "\n",
    "# Verify HK counter is always increasing\n",
    "verify['HK_Cnt'] = hs['HK_Cnt'].diff() < 0\n",
    "err_df = hs[verify['HK_Cnt']]\n",
    "if err_df.shape[0] != 0:\n",
    "    print(\"HS HK Count not increasing, %d occurances\", err_df.shape[0])\n",
    "    for row in err_df.index.values:\n",
    "        print(\"Occurance\")\n",
    "        print(hs.iloc[row-1:row+2])\n",
    "\n",
    "# Verify Sci Address is always 0xC0 00 00 00\n",
    "# Check first value is as expected\n",
    "verify['Sci_Addr'] = ~(hs['Sci_Addr'] == pc_sci_addr)\n",
    "err_df = hs[verify['Sci_Addr']]\n",
    "if err_df.shape[0] != 0:\n",
    "    print(\"HS Sci Address not as expected %d occurances\", err_df.shape[0])\n",
    "    print(err_df)\n",
    "\n",
    "# Verify the Sci Length is always 2,097,200 bytes\n",
    "verify['Sci_Len'] = ~hs['Sci_Len'].isin(pc_sci_len)\n",
    "err_df = hs[verify['Sci_Len']]\n",
    "if err_df.shape[0] != 0:\n",
    "    print(\"HS Sci unexpected length %d occurances\", err_df.shape[0])\n",
    "    print(err_df) \n",
    "\n",
    "# Verify the LDT Ctrl is always <7\n",
    "verify['LDT'] = ~(hs['LDT'] < 8)\n",
    "err_df = hs[verify['LDT']]\n",
    "if err_df.shape[0] != 0:\n",
    "    print(\"HS LDT not in expected range of 0 to 7, %d occurances\", err_df.shape[0])\n",
    "    print(err_df)\n",
    "\n",
    "# Check that the Sci counter is always increasing\n",
    "verify['Sci_Cnt'] = hs['Sci_Cnt'].diff() < 0\n",
    "err_df = hs[verify['Sci_Cnt']]\n",
    "if err_df.shape[0] != 0:\n",
    "    print(\"HS Sci count not increasing, %d occurances\", err_df.shape[0])\n",
    "    for row in err_df.index.values:\n",
    "        print(\"Occurance\")\n",
    "        print(hs.iloc[row-1:row+2])\n",
    "\n",
    "# Count number of images generated\n",
    "# First find last count\n",
    "img_cnt = hs['Sci_Cnt'].iloc[-1]\n",
    "\n",
    "# Find all instances where it has reset and add to total\n",
    "cnt_rst = hs.Sci_Cnt[hs['Sci_Cnt'].diff(-1) > 0]\n",
    "img_cnt += cnt_rst.sum()\n",
    "print(\"A total of %d images generated over session\", img_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3\n"
    }
   ],
   "source": [
    "ffcnt_rst = hs.Sci_Cnt[hs['Sci_Cnt'].diff(-1) >0]\n",
    "print(cnt_rst.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Number of lines =  14\n"
    }
   ],
   "source": [
    "proc_dir = Path(r\"C:\\Users\\ucasbwh\\Desktop\\NSVF Dev\\PROC\")\n",
    "\n",
    "sci_file = PC_Fns.Find_Files(proc_dir, 'Sci.txt')[0]\n",
    "num_lines = sum(1 for line in open(sci_file))\n",
    "print(\"Number of lines = \", num_lines)\n",
    "\n",
    "write_file = sci_file.parent / \"RAW.bin\"\n",
    "f = open(write_file, 'w+b')\n",
    "\n",
    "with open(sci_file) as txtfile:\n",
    "    reader = csv.reader(txtfile, delimiter=' ')\n",
    "    for row in reader:\n",
    "        test = row[16:-2]\n",
    "        output = ''.join(test)\n",
    "        binary_format = bytearray.fromhex(output)\n",
    "        f.write(binary_format)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0, 1]\n00\n"
    }
   ],
   "source": [
    "num_imgs = 2\n",
    "img_name = list(range(num_imgs))\n",
    "print(img_name)\n",
    "print(str(img_name[0]).zfill(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2\nFiles do not match\nFiles Match\n"
    }
   ],
   "source": [
    "## Compare science files\n",
    "proc_dir = Path(r\"C:\\Users\\ucasbwh\\Desktop\\NSVF Dev\\PROC\")\n",
    "sci_files = PC_Fns.Find_Files(proc_dir, '*.pci_raw')\n",
    "print(len(sci_files))\n",
    "\n",
    "ref_dir = Path().parent.absolute().parent / \"data\" / \"SWIS_Reference\"\n",
    "\n",
    "for sci in sci_files:\n",
    "    #Chop off header\n",
    "    gen = open(sci, 'rb')\n",
    "    header = gen.read(48)\n",
    "    cam =  upf('u2', header, offset=130)[0]   \n",
    "\n",
    "    # Determine Cam Type\n",
    "    if cam == 1:\n",
    "        # WACL Image Reference\n",
    "        ref = ref_dir / \"pfm_wacl.raw\"\n",
    "    else:\n",
    "        print(\"Wrong Type\")\n",
    "    \n",
    "    #Generate temporary file\n",
    "    tmp_file = sci.with_suffix(\".tmp\")\n",
    "    tmp = open(tmp_file, 'wb')\n",
    "    tmp.write(gen.read())\n",
    "    tmp.close()\n",
    "\n",
    "    #Compare files\n",
    "    if filecmp.cmp(tmp_file, ref, shallow=False):\n",
    "        print(\"Files Match\")\n",
    "    else:\n",
    "        print(\"Files do not match\")\n",
    "\n",
    "    tmp_file.unlink()\n",
    "    gen.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "c:\\Users\\ucasbwh\\Documents\\GitHub\\PanCam-Data-Processing-Tools\\data\\SWIS_Reference\nTrue\n"
    }
   ],
   "source": [
    "p = Path().parent.absolute().parent / \"data\" / \"SWIS_Reference\"\n",
    "print(p)\n",
    "print(p.is_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Done\n"
    }
   ],
   "source": [
    "dir = Path(r\"C:\\Users\\ucasbwh\\Desktop\\NSVF Dev\")\n",
    "\n",
    "in_re = re.compile(r\"^\\[IN=[0-9]+\\]$\")\n",
    "sz_re = re.compile(r\"^\\[SZ=[0-9]+\\]$\")\n",
    "\n",
    "\n",
    "pc_log_addr = 0x41\n",
    "pc_hs_rowlen = 45\n",
    "\n",
    "packet_log = PC_Fns.Find_Files(dir, \"Router_A_packet.log\", SingleFile=True)\n",
    "\n",
    "packet_log = packet_log[0]\n",
    "\n",
    "\n",
    "# Next prepare files to be written\n",
    "file = {}\n",
    "f_acc = {}\n",
    "f_wri = {}\n",
    "\n",
    "file['hs'] = proc_dir / 'H+S.txt'\n",
    "file['hk'] = proc_dir / 'nsvfHK.txt'\n",
    "file['tc'] = proc_dir / 'TC.txt'\n",
    "file['sc'] = proc_dir / 'Sci.txt'\n",
    "\n",
    "for key, value in file.items():\n",
    "    if value.exists():\n",
    "        value.unlink()\n",
    "    f_acc[key] = open(value, 'w')\n",
    "    f_wri[key] = csv.writer(f_acc[key], delimiter=' ', lineterminator='\\r')\n",
    "\n",
    "with open(packet_log, 'r') as logfile:\n",
    "    reader = csv.reader(logfile, delimiter=' ')\n",
    "    for row in reader:\n",
    "        # First check that row ends in [EOP]\n",
    "        if row[-1] != '[EOP]':\n",
    "            logger.error(\"Row does not end in '[EOP]': %s\", row)\n",
    "            continue\n",
    "\n",
    "        # Verify row contains [IN=..] in correct position\n",
    "        if not in_re.match(row[1]):\n",
    "            logger.error(\"Row no match for '[IN..]': %s\", row)\n",
    "            continue\n",
    "\n",
    "        # Verify row contains [SZ=..] in correct position\n",
    "        if not sz_re.match(row[2]):\n",
    "            logger.error(\"Row no match for '[SZ..]': %s\", row)\n",
    "            continue\n",
    "\n",
    "        # Verify row size matches that stated in [SZ=..]\n",
    "        row_size = int(row[2][4:-1])\n",
    "        if row_size != len(row[4:-1]):\n",
    "            logger.error(\n",
    "                \"Row row does not match expected length %d bytes: %s\", row_size, row)\n",
    "            continue\n",
    "\n",
    "        # Filter by Logical address\n",
    "        log_addr = int(row[8], 16)\n",
    "\n",
    "        if log_addr == pc_log_addr:\n",
    "            # Assume 45 byte lines are H+S\n",
    "            if row_size == pc_hs_rowlen:\n",
    "                row_red = row\n",
    "                row_red[1:4] = []\n",
    "                row_red[-1:] = []\n",
    "                row_red[0] += ';'\n",
    "                f_wri['hs'].writerow(row_red)\n",
    "\n",
    "            # If 8 byte line assume TC\n",
    "            elif row_size == 8:\n",
    "                f_wri['tc'].writerow(row)\n",
    "\n",
    "            # If 85 or 101 byte line assume HK\n",
    "            elif (row_size == 85) | (row_size == 101):\n",
    "                f_wri['hk'].writerow(row)\n",
    "\n",
    "            # Else assume Sci\n",
    "            else:\n",
    "                f_wri['sc'].writerow(row)\n",
    "\n",
    "for key, value in f_acc.items():\n",
    "    value.close()\n",
    "\n",
    "# Create a H&S Pickle File\n",
    "hs_head = ['Time', 'RAW']\n",
    "hs = pd.read_csv(file['hs'], sep=';', header=None, names=hs_head)\n",
    "hs.to_pickle(proc_dir / \"hs_raw.pickle\")\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "297.158\nSucess\n"
    }
   ],
   "source": [
    "## HK Get epoch and rename file\n",
    "dir = Path(r\"C:\\Users\\ucasbwh\\Desktop\\NSVF Dev\")\n",
    "\n",
    "nsvf_hk = PC_Fns.Find_Files(dir, \"nsvfHK.txt\", SingleFile=True)[0]\n",
    "log_file = PC_Fns.Find_Files(dir, \"logbook.log\", SingleFile=True)[0]\n",
    "\n",
    "with open(nsvf_hk, 'r') as hk:\n",
    "    reader = csv.reader(hk, delimiter=' ')\n",
    "    hk_first = next(reader)\n",
    "    hk_first_time = hk_first[0][1:-1].split(':')\n",
    "    elapsed = hk_first_time[0] + '.' + hk_first_time[1]\n",
    "print(elapsed)\n",
    "\n",
    "# Search through log file\n",
    "with open(log_file, 'r') as log:\n",
    "    reader = csv.reader(log, delimiter= ' ')\n",
    "    for row in reader:\n",
    "        try:\n",
    "            if elapsed in row[0]:\n",
    "                print(\"Sucess\")\n",
    "                epoch_str = row[3][6:].split('.')[0]\n",
    "                break\n",
    "        except:\n",
    "            None\n",
    "\n",
    "nsvf_hk.rename(nsvf_hk.parent / ('nsvfHK_Unix' + epoch_str + '.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                     0  \\\n0  [297:158:018:346] [IN=05] [SZ=0101]   \n1  [298:156:018:346] [IN=05] [SZ=0101]   \n2  [299:154:018:346] [IN=05] [SZ=0101]   \n3  [300:152:015:786] [IN=05] [SZ=0085]   \n4  [301:152:018:346] [IN=05] [SZ=0101]   \n\n                                                   1                ElapLi  \n0  21 01 0C 00 41 00 58 00 00 00 58 F2 B5 07 00 0...  [297, 158, 018, 346]  \n1  21 01 0C 00 41 00 72 00 00 00 58 5C DD 07 00 0...  [298, 156, 018, 346]  \n2  21 01 0C 00 41 00 8C 00 00 00 58 29 9D 07 00 0...  [299, 154, 018, 346]  \n3  21 01 0C 00 41 00 A6 00 00 00 48 9B F5 03 00 0...  [300, 152, 015, 786]  \n4  21 01 0C 00 41 00 C0 00 00 00 58 60 1D 07 00 0...  [301, 152, 018, 346]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>ElapLi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[297:158:018:346] [IN=05] [SZ=0101]</td>\n      <td>21 01 0C 00 41 00 58 00 00 00 58 F2 B5 07 00 0...</td>\n      <td>[297, 158, 018, 346]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[298:156:018:346] [IN=05] [SZ=0101]</td>\n      <td>21 01 0C 00 41 00 72 00 00 00 58 5C DD 07 00 0...</td>\n      <td>[298, 156, 018, 346]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[299:154:018:346] [IN=05] [SZ=0101]</td>\n      <td>21 01 0C 00 41 00 8C 00 00 00 58 29 9D 07 00 0...</td>\n      <td>[299, 154, 018, 346]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[300:152:015:786] [IN=05] [SZ=0085]</td>\n      <td>21 01 0C 00 41 00 A6 00 00 00 48 9B F5 03 00 0...</td>\n      <td>[300, 152, 015, 786]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[301:152:018:346] [IN=05] [SZ=0101]</td>\n      <td>21 01 0C 00 41 00 C0 00 00 00 58 60 1D 07 00 0...</td>\n      <td>[301, 152, 018, 346]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "swis_dir = Path(r\"C:\\Users\\ucasbwh\\Desktop\\NSVF Dev\\PROC\")\n",
    "hk_files = PC_Fns.Find_Files(swis_dir, \"*nsvfHK*.txt\")\n",
    "\n",
    "for curfile in hk_files:\n",
    "    dl = pd.DataFrame()\n",
    "    dtab = pd.read_table(curfile, sep=' : ', header=None)\n",
    "    dl['SPW_RAW'] = dtab[1].apply(lambda x: x.replace(' ',''))\n",
    "    dl['RAW'] = dl.SPW_RAW.apply(lambda x: x[24:-7])\n",
    "    dl['Source'] = 'NSVF'\n",
    "    epoch = int(curfile.stem.split('_')[1][4:])\n",
    "    dtab['ElapLi'] = dtab[0].apply(lambda x: x.split(' ')[0][1:-1].split(':'))\n",
    "\n",
    "    verify = dtab['ElapLi'].str.len() != 4\n",
    "    err_df = dtab[verify]\n",
    "    if err_df.shape[0] != 0:\n",
    "        print(\"You've got an error\")\n",
    "\n",
    "dtab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                     0  \\\n0  [297:158:018:346] [IN=05] [SZ=0101]   \n1  [298:156:018:346] [IN=05] [SZ=0101]   \n2  [299:154:018:346] [IN=05] [SZ=0101]   \n3  [300:152:015:786] [IN=05] [SZ=0085]   \n4  [301:152:018:346] [IN=05] [SZ=0101]   \n\n                                                   1                ElapLi  \\\n0  21 01 0C 00 41 00 58 00 00 00 58 F2 B5 07 00 0...  [297, 158, 018, 346]   \n1  21 01 0C 00 41 00 72 00 00 00 58 5C DD 07 00 0...  [298, 156, 018, 346]   \n2  21 01 0C 00 41 00 8C 00 00 00 58 29 9D 07 00 0...  [299, 154, 018, 346]   \n3  21 01 0C 00 41 00 A6 00 00 00 48 9B F5 03 00 0...  [300, 152, 015, 786]   \n4  21 01 0C 00 41 00 C0 00 00 00 58 60 1D 07 00 0...  [301, 152, 018, 346]   \n\n      Elapsed     Unix_Time  \n0  297.158018  1.580410e+09  \n1  298.156018  1.580410e+09  \n2  299.154018  1.580410e+09  \n3  300.152016  1.580410e+09  \n4  301.152018  1.580410e+09  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>ElapLi</th>\n      <th>Elapsed</th>\n      <th>Unix_Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[297:158:018:346] [IN=05] [SZ=0101]</td>\n      <td>21 01 0C 00 41 00 58 00 00 00 58 F2 B5 07 00 0...</td>\n      <td>[297, 158, 018, 346]</td>\n      <td>297.158018</td>\n      <td>1.580410e+09</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[298:156:018:346] [IN=05] [SZ=0101]</td>\n      <td>21 01 0C 00 41 00 72 00 00 00 58 5C DD 07 00 0...</td>\n      <td>[298, 156, 018, 346]</td>\n      <td>298.156018</td>\n      <td>1.580410e+09</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[299:154:018:346] [IN=05] [SZ=0101]</td>\n      <td>21 01 0C 00 41 00 8C 00 00 00 58 29 9D 07 00 0...</td>\n      <td>[299, 154, 018, 346]</td>\n      <td>299.154018</td>\n      <td>1.580410e+09</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[300:152:015:786] [IN=05] [SZ=0085]</td>\n      <td>21 01 0C 00 41 00 A6 00 00 00 48 9B F5 03 00 0...</td>\n      <td>[300, 152, 015, 786]</td>\n      <td>300.152016</td>\n      <td>1.580410e+09</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[301:152:018:346] [IN=05] [SZ=0101]</td>\n      <td>21 01 0C 00 41 00 C0 00 00 00 58 60 1D 07 00 0...</td>\n      <td>[301, 152, 018, 346]</td>\n      <td>301.152018</td>\n      <td>1.580410e+09</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "#dtab['Elapsed_List'][0] = dtab[0][0][1:]\n",
    "verify = dtab['ElapLi'].str.len() != 4\n",
    "err_df = dtab[verify]\n",
    "if err_df.shape[0] != 0:\n",
    "    print(\"You've got an error\")\n",
    "\n",
    "dtab['Elapsed'] = dtab['ElapLi'].apply(lambda x: float(x[0] + '.' + ''.join(x[1:])))\n",
    "dtab.head()\n",
    "dtab['Unix_Time'] = dtab['Elapsed'] + epoch\n",
    "dtab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['297', '158', '018', '346']\n297.158018346\n1580410088.1580184\n"
    }
   ],
   "source": [
    "test = dtab[0][0].split(' ')[0][1:-1].split(':')\n",
    "if len(test) != 4:\n",
    "    print(\"Warning wrong length\")\n",
    "print(test)\n",
    "elapsed = float(test[0] + '.' + ''.join(test[1:]))\n",
    "print(elapsed)\n",
    "unixtime = epoch + elapsed\n",
    "print(unixtime)"
   ]
  }
 ]
}